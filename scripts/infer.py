# scripts/infer.py
import os
import sys
import re
import torch
import json
import random
import argparse
from tqdm import tqdm
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)
from peft import LoraConfig, TaskType, get_peft_model, PeftModel
from scripts.char_adapter import CharacterAwareAdapter, load_decompose_map

# === è·¯å¾„é…ç½® ===
MODEL_PATH = "./.hf_cache_Qwen3-0.6B"
FINETUNED_MODEL_DIR = "outputs/models/qwen3-riddle-caa-lora"
TEST_DATA_PATH = "data/processed/test.json"
SRC_DATA_DIR = "data/raw/src_data"
OUTPUT_DIR = "outputs/predictions"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === ç”Ÿæˆé…ç½® ===
REPETITION_PENALTY = 1.2


# === æ¨¡å‹åŒ…è£…å™¨ï¼ˆå‚è€ƒ train.pyï¼‰ ===
class QwenWithCAA(torch.nn.Module):
    """
    å°† Qwen æ¨¡å‹ä¸ CAA (Character-Aware Adapter) ç»“åˆçš„åŒ…è£…å™¨
    å‚è€ƒ train.py çš„å®ç°
    """
    def __init__(self, base_model, caa_adapter, tokenizer):
        super().__init__()
        self.model = base_model
        self.caa = caa_adapter
        self.tokenizer = tokenizer

    def forward(self, input_ids, labels=None, target_chars=None, repetition_penalty=1.0, **kwargs):
        """å‰å‘ä¼ æ’­ï¼Œæ”¯æŒCAAæ³¨å…¥"""
        # è·å– hidden states
        outputs = self.model.model(
            input_ids=input_ids,
            output_hidden_states=True,
            **kwargs
        )
        
        # æå–æœ€åä¸€å±‚ hidden states
        if hasattr(outputs, 'last_hidden_state'):
            hidden_states = outputs.last_hidden_state
        elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
            hidden_states = outputs.hidden_states[-1]
        else:
            hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs

        # å¦‚æœæä¾›äº† target_charsï¼Œæ³¨å…¥ CAA ä¿¡æ¯
        if target_chars is not None and labels is not None:
            answer_positions = []
            for label_row in labels:
                non_ignore = (label_row != -100).nonzero(as_tuple=True)[0]
                pos = non_ignore[-1].item() if len(non_ignore) > 0 else label_row.size(0) - 1
                answer_positions.append(pos)
            hidden_states = self.caa.inject_at_positions(hidden_states, target_chars, answer_positions)

        logits = self.model.lm_head(hidden_states)
        loss = None

        if labels is not None:
            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

        return {"loss": loss, "logits": logits} if loss is not None else {"logits": logits}

    def generate(self, input_ids=None, **kwargs):
        """å§”æ‰˜ç»™åŸºç¡€æ¨¡å‹çš„generateæ–¹æ³•"""
        if input_ids is not None:
            return self.model.generate(input_ids=input_ids, **kwargs)
        else:
            return self.model.generate(**kwargs)


def load_finetuned_model():
    """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹ï¼ˆLoRA + CAAï¼‰ï¼Œå‚è€ƒ train.py çš„å®ç°æ–¹å¼"""
    print("=" * 60)
    print("æ­£åœ¨åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...")
    print("=" * 60)
    
    # æ£€æŸ¥å¿…è¦çš„è·¯å¾„
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_PATH}")
    if not os.path.exists(FINETUNED_MODEL_DIR):
        raise FileNotFoundError(f"å¾®è°ƒæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {FINETUNED_MODEL_DIR}")
    if not os.path.exists(SRC_DATA_DIR):
        raise FileNotFoundError(f"æºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {SRC_DATA_DIR}")
    
    # åŠ è½½tokenizer
    print("\n[1/5] åŠ è½½tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            fix_mistral_regex=True
        )
        tokenizer.pad_token = tokenizer.eos_token
        print("    âœ… Tokenizer åŠ è½½æˆåŠŸ")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½ tokenizer å¤±è´¥: {e}")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print("[2/5] åŠ è½½åŸºç¡€æ¨¡å‹...")
    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            dtype=torch.float16,
            device_map="auto",
            use_cache=True
        )
        base_model.eval()
        print("    âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½åŸºç¡€æ¨¡å‹å¤±è´¥: {e}")
    
    # é…ç½®å¹¶åŠ è½½LoRAï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶é…ç½®ä¸€è‡´ï¼‰
    print("[3/5] é…ç½®å¹¶åŠ è½½LoRAé€‚é…å™¨...")
    lora_config = LoraConfig(
        r=16,  # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
        lora_alpha=32,  # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    finetuned_model = get_peft_model(base_model, lora_config)
    
    # åŠ è½½LoRAæƒé‡
    lora_dir = os.path.join(FINETUNED_MODEL_DIR, "lora")
    
    # ä¼˜å…ˆå°è¯•ä½¿ç”¨ PeftModel.from_pretrained åŠ è½½
    if os.path.exists(lora_dir):
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ adapter_config.json
            adapter_config_path = os.path.join(lora_dir, "adapter_config.json")
            if os.path.exists(adapter_config_path):
                print(f"    ä» {lora_dir} åŠ è½½LoRAé€‚é…å™¨...")
                finetuned_model = PeftModel.from_pretrained(base_model, lora_dir)
                print("    âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨PeftModel.from_pretrainedï¼‰")
            else:
                # å¦‚æœæ²¡æœ‰adapter_config.jsonï¼Œå°è¯•æ‰‹åŠ¨åŠ è½½æƒé‡
                weights_file = os.path.join(lora_dir, "pytorch_model.bin")
                if os.path.exists(weights_file):
                    print(f"    ä» {weights_file} åŠ è½½LoRAæƒé‡...")
                    state_dict = torch.load(weights_file, map_location="cpu")
                    missing_keys, unexpected_keys = finetuned_model.load_state_dict(state_dict, strict=False)
                    loaded_lora_keys = [k for k in state_dict.keys() 
                                      if any(lora_key in k for lora_key in ["lora_A", "lora_B", "lora_embedding"])]
                    if loaded_lora_keys:
                        print(f"    âœ… å·²åŠ è½½ {len(loaded_lora_keys)} ä¸ªLoRAå‚æ•°")
                    else:
                        print("    âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°LoRAæƒé‡")
                else:
                    # å°è¯•ä»æœ€æ–°çš„checkpointåŠ è½½
                    checkpoint_dirs = []
                    if os.path.exists(FINETUNED_MODEL_DIR):
                        for item in os.listdir(FINETUNED_MODEL_DIR):
                            checkpoint_path = os.path.join(FINETUNED_MODEL_DIR, item)
                            if os.path.isdir(checkpoint_path) and item.startswith("checkpoint-"):
                                checkpoint_dirs.append((item, checkpoint_path))
                    
                    if checkpoint_dirs:
                        checkpoint_dirs.sort(key=lambda x: int(x[0].split("-")[1]), reverse=True)
                        latest_checkpoint = checkpoint_dirs[0][1]
                        checkpoint_weights = os.path.join(latest_checkpoint, "pytorch_model.bin")
                        if os.path.exists(checkpoint_weights):
                            print(f"    ä» {checkpoint_weights} åŠ è½½LoRAæƒé‡...")
                            state_dict = torch.load(checkpoint_weights, map_location="cpu")
                            missing_keys, unexpected_keys = finetuned_model.load_state_dict(state_dict, strict=False)
                            loaded_lora_keys = [k for k in state_dict.keys() 
                                              if any(lora_key in k for lora_key in ["lora_A", "lora_B", "lora_embedding"])]
                            if loaded_lora_keys:
                                print(f"    âœ… å·²åŠ è½½ {len(loaded_lora_keys)} ä¸ªLoRAå‚æ•°")
                            else:
                                print("    âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°LoRAæƒé‡")
                        else:
                            print(f"    âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°LoRAæƒé‡æ–‡ä»¶")
                    else:
                        print(f"    âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°LoRAæƒé‡æ–‡ä»¶")
        except Exception as e:
            print(f"    âŒ åŠ è½½LoRAæƒé‡æ—¶å‡ºé”™: {e}")
            print("    å°†ä½¿ç”¨æœªåŠ è½½æƒé‡çš„LoRAæ¨¡å‹")
    else:
        print(f"    âš ï¸  è­¦å‘Š: LoRAç›®å½•ä¸å­˜åœ¨: {lora_dir}")
    
    finetuned_model.eval()
    
    # åŠ è½½CAAé€‚é…å™¨
    print("[4/5] åŠ è½½CAAé€‚é…å™¨...")
    caa_path = os.path.join(FINETUNED_MODEL_DIR, "caa.bin")
    decompose_map = load_decompose_map(SRC_DATA_DIR)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    caa = CharacterAwareAdapter(
        decompose_map,
        embed_dim=256,
        hidden_size=finetuned_model.config.hidden_size,
        device=device
    )
    
    if os.path.exists(caa_path):
        caa.load_state_dict(torch.load(caa_path, map_location="cpu"))
        print(f"    âœ… å·²åŠ è½½CAAæƒé‡: {caa_path}")
    else:
        print(f"    âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°CAAæƒé‡æ–‡ä»¶ {caa_path}")
    
    caa = caa.to(device)
    caa.eval()
    
    # åŒ…è£…æ¨¡å‹ï¼ˆå‚è€ƒ train.pyï¼‰
    print("[5/5] åŒ…è£…æ¨¡å‹...")
    model_with_caa = QwenWithCAA(finetuned_model, caa, tokenizer)
    model_with_caa.eval()
    
    print("\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return tokenizer, model_with_caa


def prepare_prompt(riddle: str, clue: str) -> str:
    """å‡†å¤‡æ¨ç†ç”¨çš„promptï¼Œå‚è€ƒ train.py çš„æ ¼å¼"""
    prompt = (
        "<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªå­—è°œä¸“å®¶ã€‚<|im_end|>\n"
        "<|im_start|>user\n"
        f"è°œé¢ï¼š{riddle}\n"
        f"çº¿ç´¢ï¼š{clue}\n"
        "è¯·ç›´æ¥å›ç­”ä¸€ä¸ªæ±‰å­—ã€‚<|im_end|>\n"
        "<|im_start|>assistant\n"
    )
    return prompt


def extract_answer(generated_text: str) -> str:
    """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–ç­”æ¡ˆæ±‰å­—ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºè°ƒè¯•
    original_text = generated_text
    
    # æ¸…ç†ç”Ÿæˆçš„æ–‡æœ¬ï¼šç§»é™¤ç‰¹æ®Šæ ‡è®°
    generated_text = generated_text.strip()
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šé¦–å…ˆç§»é™¤æ¨ç†ç›¸å…³çš„XMLæ ‡ç­¾åŠå…¶å†…å®¹ï¼ˆä½¿ç”¨DOTALLæ¨¡å¼ï¼‰
    # è¿™æ ·å¯ä»¥ç§»é™¤æ•´ä¸ªæ¨ç†å—ï¼Œé¿å…å½±å“ç­”æ¡ˆæå–
    generated_text = re.sub(r'<think>.*?</think>', '', generated_text, flags=re.DOTALL)
    generated_text = re.sub(r'<reasoning>.*?</reasoning>', '', generated_text, flags=re.DOTALL)
    
    # ç§»é™¤å…¶ä»–XMLé£æ ¼çš„æ ‡è®°ï¼ˆå•ä¸ªæ ‡ç­¾ï¼‰
    generated_text = re.sub(r'<[^>]+>', '', generated_text)
    
    # ç§»é™¤å¸¸è§çš„ç‰¹æ®Šæ ‡è®°è¯
    special_markers = ['think', 'reasoning', 'redacted', 'assistant', 'user', 'system']
    for marker in special_markers:
        generated_text = generated_text.replace(marker, '')
    
    # æ¸…ç†å¤šä½™çš„ç©ºç™½è¡Œ
    generated_text = re.sub(r'\n\s*\n+', '\n', generated_text)
    generated_text = generated_text.strip()
    
    # æå–æ‰€æœ‰ä¸­æ–‡å­—ç¬¦
    chinese_chars = [char for char in generated_text if '\u4e00' <= char <= '\u9fff']
    
    if not chinese_chars:
        return ""
    
    prediction = ""
    
    # ç­–ç•¥1: æŸ¥æ‰¾ç­”æ¡ˆæç¤ºè¯åçš„ç¬¬ä¸€ä¸ªä¸­æ–‡å­—ç¬¦ï¼ˆæ›´å…¨é¢çš„æ¨¡å¼ï¼‰
    answer_patterns = [
        r'æœ€ç»ˆç­”æ¡ˆä¸º[ï¼š:ï¼š]?\s*[ï¼š:ï¼š\n]*\s*([\u4e00-\u9fff])',  # "æœ€ç»ˆç­”æ¡ˆä¸ºï¼š"ã€"æœ€ç»ˆç­”æ¡ˆä¸ºï¼š\n\nç‰›"
        r'æœ€ç»ˆç­”æ¡ˆ[ï¼š:ï¼š]?\s*[ï¼š:ï¼š\n]*\s*([\u4e00-\u9fff])',  # "æœ€ç»ˆç­”æ¡ˆï¼š"
        r'ç­”æ¡ˆæ˜¯[ï¼š:ï¼š]?\s*([\u4e00-\u9fff])',  # "ç­”æ¡ˆæ˜¯ï¼š"ã€"ç­”æ¡ˆæ˜¯ï¼šç‰›"
        r'ç­”æ¡ˆ[ï¼š:ï¼š]?\s*[ï¼š:ï¼š\n]*\s*([\u4e00-\u9fff])',  # "ç­”æ¡ˆï¼š"ã€"ç­”æ¡ˆï¼š\nç‰›"
        r'ä¸º[ï¼š:ï¼š]?\s*([\u4e00-\u9fff])',  # "ä¸ºï¼š"ã€"ä¸º ç‰›"
        r'åº”ä¸º[ï¼š:ï¼š]?\s*([\u4e00-\u9fff])',  # "åº”ä¸ºï¼š"
        r'æ˜¯[ï¼š:ï¼š]?\s*([\u4e00-\u9fff])',  # "æ˜¯ï¼š"
        r'ï¼š\s*([\u4e00-\u9fff])',  # "ï¼šç‰›"ï¼ˆå†’å·åçš„ç¬¬ä¸€ä¸ªæ±‰å­—ï¼‰
    ]
    
    for pattern in answer_patterns:
        matches = re.finditer(pattern, generated_text)
        for match in matches:
            char = match.group(1)
            # ç¡®ä¿æå–çš„æ˜¯å•ä¸ªæ±‰å­—
            if len(char) == 1 and '\u4e00' <= char <= '\u9fff':
                prediction = char
                break
        if prediction:
            break
    
    # ç­–ç•¥2: æŸ¥æ‰¾markdownæ ¼å¼çš„ç­”æ¡ˆï¼ˆ**ç­”æ¡ˆ**ã€*ç­”æ¡ˆ*ã€ã€ç­”æ¡ˆã€‘ç­‰ï¼‰
    if not prediction:
        markdown_patterns = [
            r'\*\*([\u4e00-\u9fff])\*\*',  # **ç‰›**
            r'\*([\u4e00-\u9fff])\*',  # *ç‰›*
            r'ã€([\u4e00-\u9fff])ã€‘',  # ã€ç‰›ã€‘
            r'\[([\u4e00-\u9fff])\]',  # [ç‰›]
            r'ï¼ˆ([\u4e00-\u9fff])ï¼‰',  # ï¼ˆç‰›ï¼‰
            r'\(([\u4e00-\u9fff])\)',  # (ç‰›)
        ]
        for pattern in markdown_patterns:
            matches = re.finditer(pattern, generated_text)
            for match in matches:
                char = match.group(1)
                if len(char) == 1 and '\u4e00' <= char <= '\u9fff':
                    prediction = char
                    break
            if prediction:
                break
    
    # ç­–ç•¥3: æŸ¥æ‰¾æ–‡æœ¬æœ«å°¾çš„ç­”æ¡ˆï¼ˆé€šå¸¸ç­”æ¡ˆåœ¨æœ€åï¼‰
    if not prediction:
        # è·å–æ–‡æœ¬çš„æœ€å100ä¸ªå­—ç¬¦ï¼Œç­”æ¡ˆé€šå¸¸åœ¨è¿™é‡Œ
        text_tail = generated_text[-100:] if len(generated_text) > 100 else generated_text
        tail_chinese = [char for char in text_tail if '\u4e00' <= char <= '\u9fff']
        if tail_chinese:
            # å®šä¹‰åˆ†éš”ç¬¦é›†åˆï¼ˆæ ‡ç‚¹ã€ç©ºæ ¼ã€æ¢è¡Œç­‰ï¼‰
            separators = set([' ', '\n', '\t', 'ï¼š', ':', 'ï¼Œ', ',', 'ã€‚', '.', 'ã€', 'ï¼›', ';', 
                            'ï¼', '!', 'ï¼Ÿ', '?', 'ã€', '[', 'ã€‘', ']', 'ï¼ˆ', '(', 'ï¼‰', ')', 
                            '<', '|', '|', '>', 'ã€Š', 'ã€‹', 'ã€Œ', 'ã€'])
            # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªç‹¬ç«‹çš„æ±‰å­—ï¼ˆå‰åæœ‰åˆ†éš”ç¬¦ï¼‰
            for i in range(len(text_tail) - 1, -1, -1):
                char = text_tail[i]
                if '\u4e00' <= char <= '\u9fff':
                    # æ£€æŸ¥å‰åå­—ç¬¦ï¼Œç¡®ä¿æ˜¯ç‹¬ç«‹çš„æ±‰å­—
                    prev_char = text_tail[i-1] if i > 0 else ' '
                    next_char = text_tail[i+1] if i < len(text_tail) - 1 else ' '
                    # å¦‚æœå‰åæ˜¯åˆ†éš”ç¬¦ï¼Œå¯èƒ½æ˜¯ç­”æ¡ˆ
                    if prev_char in separators or next_char in separators:
                        prediction = char
                        break
            # å¦‚æœæ²¡æ‰¾åˆ°ç‹¬ç«‹çš„ï¼Œå°±ç”¨æœ€åä¸€ä¸ªæ±‰å­—
            if not prediction and tail_chinese:
                prediction = tail_chinese[-1]
    
    # ç­–ç•¥4: å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æ‰€æœ‰ä¸­æ–‡å­—ç¬¦ä¸­çš„æœ€åä¸€ä¸ª
    if not prediction and chinese_chars:
        prediction = chinese_chars[-1]
    
    # ç­–ç•¥5: å¦‚æœåªæœ‰ä¸€ä¸ªä¸­æ–‡å­—ç¬¦ï¼Œç›´æ¥è¿”å›å®ƒ
    if not prediction and len(chinese_chars) == 1:
        prediction = chinese_chars[0]
    
    return prediction


def infer_single_sample(model, tokenizer, riddle: str, clue: str, max_new_tokens=1000):
    """å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œæ¨ç†"""
    # å‡†å¤‡è¾“å…¥
    prompt = prepare_prompt(riddle, clue)
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=256,
        return_attention_mask=True
    )
    
    # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
    device = next(model.model.parameters()).device
    input_ids = inputs["input_ids"].to(device)
    input_length = input_ids.shape[1]
    attention_mask = inputs.get("attention_mask", None)
    if attention_mask is not None:
        attention_mask = attention_mask.to(device)
    
    # ç”Ÿæˆç­”æ¡ˆï¼ˆä¼˜åŒ–ç”Ÿæˆå‚æ•°ï¼‰
    with torch.no_grad():
        generate_kwargs = {
            "input_ids": input_ids,
            "max_new_tokens": max_new_tokens,
            "do_sample": True,
            "temperature": 0.5,  # é™ä½temperatureï¼Œä½¿ç”Ÿæˆæ›´ç¨³å®š
            "top_p": 0.9,  # æ·»åŠ nucleus sampling
            "repetition_penalty": REPETITION_PENALTY,
            "pad_token_id": tokenizer.pad_token_id,
            "eos_token_id": tokenizer.eos_token_id,
            "no_repeat_ngram_size": 3,  # é¿å…é‡å¤çš„3-gram
        }
        if attention_mask is not None:
            generate_kwargs["attention_mask"] = attention_mask
        
        generated_ids = model.generate(**generate_kwargs)
    
    # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
    new_tokens = generated_ids[0][input_length:]
    generated_text = tokenizer.decode(new_tokens, skip_special_tokens=False)
    
    # æå–ç­”æ¡ˆï¼ˆå…ˆæ¸…ç†å†æå–ï¼‰
    # ç§»é™¤ç‰¹æ®Štokenæ ‡è®°ï¼Œä½†ä¿ç•™å…¶ä»–å†…å®¹ç”¨äºæå–
    cleaned_text = tokenizer.decode(new_tokens, skip_special_tokens=True)
    prediction = extract_answer(cleaned_text)
    
    # å¦‚æœç¬¬ä¸€æ¬¡æå–å¤±è´¥ï¼Œå°è¯•ä»åŸå§‹æ–‡æœ¬ä¸­æå–ï¼ˆå¯èƒ½åŒ…å«ç‰¹æ®Štokenï¼‰
    if not prediction:
        prediction = extract_answer(generated_text)
    
    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä»å®Œæ•´è¾“å‡ºä¸­æå–ï¼ˆåŒ…å«è¾“å…¥éƒ¨åˆ†ï¼‰
    if not prediction:
        full_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        # åªæå–assistantéƒ¨åˆ†ä¹‹åçš„å†…å®¹
        assistant_marker = "<|im_start|>assistant\n"
        if assistant_marker in full_output:
            assistant_part = full_output.split(assistant_marker)[-1]
            prediction = extract_answer(assistant_part)
    
    # ä¿å­˜å®Œæ•´è¾“å‡ºç”¨äºè°ƒè¯•
    full_output = tokenizer.decode(generated_ids[0], skip_special_tokens=False)
    
    return prediction, full_output


def main():
    """ä¸»å‡½æ•°ï¼šå¯¹æµ‹è¯•é›†è¿›è¡Œæ¨ç†"""
    # å£°æ˜å…¨å±€å˜é‡ï¼ˆå¿…é¡»åœ¨å‡½æ•°å¼€å§‹å¤„å£°æ˜ï¼‰
    global MODEL_PATH, FINETUNED_MODEL_DIR, TEST_DATA_PATH, OUTPUT_DIR
    
    parser = argparse.ArgumentParser(description="å¯¹æµ‹è¯•é›†è¿›è¡Œæ¨ç†")
    parser.add_argument("--num_samples", type=int, default=10, help="éšæœºæŠ½å–çš„æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤10ï¼‰")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆé»˜è®¤42ï¼‰")
    parser.add_argument("--model_path", type=str, default=MODEL_PATH, help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--finetuned_dir", type=str, default=FINETUNED_MODEL_DIR, help="å¾®è°ƒæ¨¡å‹ç›®å½•")
    parser.add_argument("--test_data", type=str, default=TEST_DATA_PATH, help="æµ‹è¯•æ•°æ®è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default=OUTPUT_DIR, help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()
    
    # æ›´æ–°å…¨å±€é…ç½®
    MODEL_PATH = args.model_path
    FINETUNED_MODEL_DIR = args.finetuned_dir
    TEST_DATA_PATH = args.test_data
    OUTPUT_DIR = args.output_dir
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("=" * 60)
    print("å¼€å§‹æ¨ç†ä»»åŠ¡")
    print("=" * 60)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print(f"\næ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {TEST_DATA_PATH}")
    with open(TEST_DATA_PATH, "r", encoding="utf-8") as f:
        test_data = json.load(f)
    print(f"æµ‹è¯•æ ·æœ¬æ€»æ•°: {len(test_data)}")
    
    # éšæœºæŠ½å–æŒ‡å®šæ•°é‡çš„æ ·æœ¬
    random.seed(args.seed)
    if len(test_data) > args.num_samples:
        sampled_data = random.sample(test_data, args.num_samples)
        print(f"éšæœºæŠ½å– {args.num_samples} ä¸ªæ ·æœ¬ï¼ˆéšæœºç§å­: {args.seed}ï¼‰")
    else:
        sampled_data = test_data
        print(f"æµ‹è¯•æ•°æ®é‡å°‘äºè¯·æ±‚æ•°é‡ï¼Œä½¿ç”¨å…¨éƒ¨ {len(test_data)} ä¸ªæ ·æœ¬")
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_finetuned_model()
    
    # è¿›è¡Œæ¨ç†
    print("\n" + "=" * 60)
    print("å¼€å§‹æ¨ç†...")
    print("=" * 60)
    
    predictions = []
    for idx, item in enumerate(tqdm(sampled_data, desc="æ¨ç†è¿›åº¦")):
        try:
            prediction, full_output = infer_single_sample(
                model, 
                tokenizer, 
                item['riddle'], 
                item['clue']
            )
            
            # ä¿å­˜ç»“æœ
            result = {
                "index": idx,
                "riddle": item['riddle'],
                "clue": item['clue'],
                "ground_truth": item['answer'],
                "prediction": prediction,
                "correct": prediction == item['answer'],
                "full_output": full_output
            }
            predictions.append(result)
            
        except Exception as e:
            print(f"\nå¤„ç†ç¬¬ {idx} ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            result = {
                "index": idx,
                "riddle": item.get('riddle', ''),
                "clue": item.get('clue', ''),
                "ground_truth": item.get('answer', ''),
                "prediction": "",
                "correct": False,
                "error": str(e)
            }
            predictions.append(result)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total = len(predictions)
    correct = sum(1 for p in predictions if p.get('correct', False))
    accuracy = correct / total if total > 0 else 0.0
    
    print("\n" + "=" * 60)
    print("æ¨ç†å®Œæˆï¼")
    print("=" * 60)
    print(f"æ€»æ ·æœ¬æ•°: {total}")
    print(f"æ­£ç¡®é¢„æµ‹: {correct}")
    print(f"å‡†ç¡®ç‡: {accuracy:.4f} ({correct}/{total})")
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(OUTPUT_DIR, f"predictions_{timestamp}.json")
    
    output_data = {
        "metadata": {
            "timestamp": timestamp,
            "total_samples": total,
            "correct_predictions": correct,
            "accuracy": accuracy,
            "model_dir": FINETUNED_MODEL_DIR,
            "num_samples": args.num_samples,
            "seed": args.seed,
        },
        "predictions": predictions
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    
    # åŒæ—¶ä¿å­˜ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼ˆä¸åŒ…å«full_outputï¼‰
    simplified_file = os.path.join(OUTPUT_DIR, f"predictions_simplified_{timestamp}.json")
    simplified_data = {
        "metadata": output_data["metadata"],
        "predictions": [
            {k: v for k, v in pred.items() if k != "full_output"}
            for pred in predictions
        ]
    }
    
    with open(simplified_file, "w", encoding="utf-8") as f:
        json.dump(simplified_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç®€åŒ–ç‰ˆç»“æœå·²ä¿å­˜è‡³: {simplified_file}")


if __name__ == "__main__":
    main()
