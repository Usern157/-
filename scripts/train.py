# scripts/train.py
"""
è®­ç»ƒè„šæœ¬ï¼šä½¿ç”¨ LoRA + CAA (Character-Aware Adapter) å¾®è°ƒ Qwen3 æ¨¡å‹ç”¨äºå­—è°œä»»åŠ¡

ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ è½½å¹¶é¢„å¤„ç†å­—è°œæ•°æ®é›†
2. ä½¿ç”¨ LoRA è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
3. ä½¿ç”¨ CAA æ³¨å…¥å­—ç¬¦ç»“æ„ä¿¡æ¯
4. è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹

ä½œè€…ï¼šNLP é¡¹ç›®ç»„
æ—¥æœŸï¼š2024
"""
import os
import sys
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    WEIGHTS_NAME,
    EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import json
import numpy as np

from scripts.char_adapter import CharacterAwareAdapter, load_decompose_map

# === è·¯å¾„é…ç½® ===
MODEL_PATH = "./.hf_cache_Qwen3-0.6B"
DATA_DIR = "data/processed"
OUTPUT_DIR = "outputs/models/qwen3-riddle-caa-lora"
SRC_DATA_DIR = "data/raw/src_data"

# === è®­ç»ƒé…ç½® ===
# é‡å¤æŠ‘åˆ¶å‚æ•°ï¼ˆç”¨äºè®­ç»ƒæ—¶é¿å…å­¦ä¹ é‡å¤æ¨¡å¼ï¼Œé»˜è®¤å…³é—­ï¼‰
# æ³¨æ„ï¼šé‡å¤æŠ‘åˆ¶ä¸»è¦åº”åœ¨ç”Ÿæˆæ—¶ä½¿ç”¨ï¼Œè®­ç»ƒæ—¶é€šå¸¸ä¸éœ€è¦
REPETITION_PENALTY = 1.0  # 1.0 è¡¨ç¤ºä¸åº”ç”¨é‡å¤æŠ‘åˆ¶ï¼Œ>1.0 è¡¨ç¤ºæƒ©ç½šé‡å¤

os.makedirs(OUTPUT_DIR, exist_ok=True)

# === è‡ªå®šä¹‰æ¨¡å‹åŒ…è£…å™¨ ===
class QwenWithCAA(torch.nn.Module):
    """
    å°† Qwen æ¨¡å‹ä¸ CAA (Character-Aware Adapter) ç»“åˆçš„åŒ…è£…å™¨
    
    åœ¨ç”Ÿæˆç­”æ¡ˆçš„ä½ç½®æ³¨å…¥å­—ç¬¦ç»“æ„ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£æ±‰å­—ç»“æ„
    """
    def __init__(self, base_model, caa_adapter, tokenizer):
        super().__init__()
        self.model = base_model
        self.caa = caa_adapter
        self.tokenizer = tokenizer

    def forward(self, input_ids, labels=None, target_chars=None, repetition_penalty=1.0, **kwargs):
        """å‰å‘ä¼ æ’­ï¼Œæ”¯æŒCAAæ³¨å…¥"""
        # è·å– hidden states
        outputs = self.model.model(
            input_ids=input_ids,
            output_hidden_states=True,
            **kwargs
        )
        
        # æå–æœ€åä¸€å±‚ hidden states
        if hasattr(outputs, 'last_hidden_state'):
            hidden_states = outputs.last_hidden_state
        elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
            hidden_states = outputs.hidden_states[-1]
        else:
            hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs

        # å¦‚æœæä¾›äº† target_charsï¼Œæ³¨å…¥ CAA ä¿¡æ¯
        if target_chars is not None:
            answer_positions = []
            for label_row in labels:
                non_ignore = (label_row != -100).nonzero(as_tuple=True)[0]
                pos = non_ignore[-1].item() if len(non_ignore) > 0 else label_row.size(0) - 1
                answer_positions.append(pos)
            hidden_states = self.caa.inject_at_positions(hidden_states, target_chars, answer_positions)

        logits = self.model.lm_head(hidden_states)
        loss = None

        if labels is not None:
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¸è¦ shiftï¼labels ä¸ input_ids å¯¹é½ï¼ˆä½ç½® t çš„ label æ˜¯ xtï¼‰
            # ä½¿ç”¨ ignore_index=-100 è‡ªåŠ¨å¿½ç•¥ prompt éƒ¨åˆ†
            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

            # ğŸ§ª è°ƒè¯•ï¼šä»…åœ¨ç¬¬ä¸€ä¸ª batch æ‰“å°ä¸€æ¬¡ï¼ˆé¿å…åˆ·å±ï¼‰
            if not hasattr(self, "_debug_logged"):
                self._debug_logged = True
                num_valid_labels = (labels != -100).sum().item()
                total_tokens = labels.numel()
                print(f"[DEBUG] Batch label stats:")
                print(f"  - Total tokens: {total_tokens}")
                print(f"  - Valid (non -100) labels: {num_valid_labels}")
                print(f"  - Loss: {loss.item():.4f}")
                if num_valid_labels > 0:
                    # è§£ç å‰å‡ ä¸ªæœ‰æ•ˆ token ç”¨äºéªŒè¯
                    valid_mask = labels[0] != -100
                    if valid_mask.any():
                        valid_ids = labels[0][valid_mask]
                        decoded = self.tokenizer.decode(valid_ids, skip_special_tokens=False)
                        print(f"  - Decoded valid labels (first sample): '{decoded}'")

        return {"loss": loss, "logits": logits} if loss is not None else {"logits": logits}

# === æ•°æ®åŠ è½½å‡½æ•° ===
def load_dataset(split):
    """åŠ è½½è®­ç»ƒ/éªŒè¯æ•°æ®é›†"""
    data_path = os.path.join(DATA_DIR, f"{split}.json")
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    with open(data_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    texts, answers = [], []
    for item in data:
        prompt = (
            "<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªå­—è°œä¸“å®¶ã€‚<|im_end|>\n"
            "<|im_start|>user\n"
            f"è°œé¢ï¼š{item['riddle']}\n"
            f"çº¿ç´¢ï¼š{item['clue']}\n"
            "è¯·ç›´æ¥å›ç­”ä¸€ä¸ªæ±‰å­—ã€‚<|im_end|>\n"
            "<|im_start|>assistant\n"
            f"{item['answer']}<|im_end|>"
        )
        texts.append(prompt)
        answers.append(item["answer"])
    
    print(f"âœ… å·²åŠ è½½ {split} æ•°æ®é›†: {len(texts)} ä¸ªæ ·æœ¬")
    return Dataset.from_dict({"text": texts, "answer_char": answers})

# === æ•°æ®æ ‡è®°åŒ–å‡½æ•° ===
def tokenize_fn(examples, tokenizer):
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=256,
        return_tensors=None
    )
    
    input_ids = tokenized["input_ids"]
    labels = []
    
    # è·å– <|im_end|> çš„ token ID
    end_token_id = tokenizer.convert_tokens_to_ids("<|im_end|>")
    if end_token_id == tokenizer.unk_token_id:
        # fallback: try encoding
        end_token_id = tokenizer.encode("<|im_end|>", add_special_tokens=False)[0]

    for ids in input_ids:
        label = [-100] * len(ids)
        try:
            # ä»åå¾€å‰æ‰¾ <|im_end|>
            end_idx = -1
            for i in range(len(ids) - 1, -1, -1):
                if ids[i] == end_token_id:
                    end_idx = i
                    break
            if end_idx != -1 and end_idx >= 1:
                # å‡è®¾ç­”æ¡ˆæ˜¯ <|im_end|> å‰çš„ä¸€ä¸ª token
                label[end_idx - 1] = ids[end_idx - 1]
                # å¯é€‰ï¼šä¹Ÿè®­ç»ƒå€’æ•°ç¬¬äºŒä¸ªï¼ˆé˜²å¤šå­—æˆ–ç©ºæ ¼ï¼‰
                if end_idx >= 2:
                    label[end_idx - 2] = ids[end_idx - 2]
        except Exception as e:
            # ä¿æŒå…¨ -100ï¼Œè¡¨ç¤ºå¿½ç•¥è¯¥ä½ç½®
            pass
        labels.append(label)
    
    return {
        "input_ids": input_ids,
        "labels": labels,
        "answer_char": examples["answer_char"]
    }

# === ä¸»è®­ç»ƒå‡½æ•° ===
def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=" * 60)
    print("å¼€å§‹è®­ç»ƒæµç¨‹")
    print("=" * 60)
    
    # æ£€æŸ¥å¿…è¦çš„ç›®å½•å’Œæ–‡ä»¶
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_PATH}")
    if not os.path.exists(DATA_DIR):
        raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
    if not os.path.exists(SRC_DATA_DIR):
        raise FileNotFoundError(f"æºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {SRC_DATA_DIR}")
    
    # ğŸ”§ ä¿®å¤1: tokenizer åŠ è½½æ—¶å¯ç”¨ regex ä¿®å¤
    print("\n[1/6] åŠ è½½ tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            fix_mistral_regex=True  # â† å…³é”®ï¼
        )
        tokenizer.pad_token = tokenizer.eos_token
        print("âœ… Tokenizer åŠ è½½æˆåŠŸ")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½ tokenizer å¤±è´¥: {e}")

    # ğŸ”§ ä¿®å¤2: ä½¿ç”¨ dtype æ›¿ä»£ torch_dtype
    print("[2/6] åŠ è½½åŸºç¡€æ¨¡å‹...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            dtype=torch.float16,  # â† æ›¿æ¢ torch_dtype
            device_map="auto",
            use_cache=False
        )
        model.gradient_checkpointing_enable()
        print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½åŸºç¡€æ¨¡å‹å¤±è´¥: {e}")

    # LoRA é…ç½®ï¼ˆç»Ÿä¸€é…ç½®ï¼Œç¡®ä¿è®­ç»ƒå’Œæ¨ç†ä¸€è‡´ï¼‰
    # æ³¨æ„ï¼šå¦‚éœ€ä¿®æ”¹ï¼Œè¯·åŒæ­¥æ›´æ–° infer.py å’Œ evaluate.py ä¸­çš„é…ç½®
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    print(f"ğŸ“Œ LoRA é…ç½®: r={lora_config.r}, alpha={lora_config.lora_alpha}, target_modules={lora_config.target_modules}")
    print("[3/6] åº”ç”¨ LoRA é€‚é…å™¨...")
    model = get_peft_model(model, lora_config)
    print("âœ… LoRA é€‚é…å™¨å·²åº”ç”¨")

    # CAA (Character-Aware Adapter) åˆå§‹åŒ–
    print("[4/6] åˆå§‹åŒ– CAA é€‚é…å™¨...")
    try:
        decompose_map = load_decompose_map(SRC_DATA_DIR)
        print(f"âœ… å·²åŠ è½½ {len(decompose_map)} ä¸ªå­—ç¬¦çš„æ‹†è§£æ˜ å°„")
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        caa = CharacterAwareAdapter(
            decompose_map,
            embed_dim=256,
            hidden_size=model.config.hidden_size,
            device=device
        ).to(device)
        print("âœ… CAA é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        raise RuntimeError(f"åˆå§‹åŒ– CAA é€‚é…å™¨å¤±è´¥: {e}")

    print("[5/6] åŒ…è£…æ¨¡å‹...")
    model_with_caa = QwenWithCAA(model, caa, tokenizer)
    model_with_caa.train()
    print("âœ… æ¨¡å‹åŒ…è£…å®Œæˆ")

    # åŠ è½½æ•°æ®
    print("[6/6] åŠ è½½æ•°æ®é›†...")
    try:
        train_ds = load_dataset("train")
        val_ds_full = load_dataset("valid")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")

    # ğŸ”§ ä¼˜åŒ–ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­åªä½¿ç”¨æå°‘éƒ¨åˆ†éªŒè¯é›†ï¼ˆåŠ å¿«éªŒè¯é€Ÿåº¦ï¼‰
    # åªä¿ç•™å‰ 10 ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„éªŒè¯ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    VAL_SUBSET_SIZE = 10
    if len(val_ds_full) > VAL_SUBSET_SIZE:
        val_ds = val_ds_full.select(range(VAL_SUBSET_SIZE))
        print(f"ğŸ“Š éªŒè¯é›†å·²ç¼©å‡: {len(val_ds_full)} -> {len(val_ds)} ä¸ªæ ·æœ¬ï¼ˆä»…ç”¨äºè®­ç»ƒè¿‡ç¨‹éªŒè¯ï¼‰")
    else:
        val_ds = val_ds_full
        print(f"ğŸ“Š éªŒè¯é›†å¤§å°: {len(val_ds)} ä¸ªæ ·æœ¬")

    train_ds = train_ds.map(
        lambda x: tokenize_fn(x, tokenizer),
        batched=True,
        remove_columns=["text"]  # ä¿ç•™ answer_charï¼Œå› ä¸ºè®­ç»ƒæ—¶éœ€è¦ç”¨åˆ°
    )
    val_ds = val_ds.map(
        lambda x: tokenize_fn(x, tokenizer),
        batched=True,
        remove_columns=["text"]  # ä¿ç•™ answer_charï¼Œå› ä¸ºè®­ç»ƒæ—¶éœ€è¦ç”¨åˆ°
    )

    class CustomDataCollator:
        """
        è‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨ï¼Œå¤„ç† answer_char å­—æ®µ
        
        åœ¨æ ‡å‡†çš„æ•°æ®æ•´ç†åŸºç¡€ä¸Šï¼Œä¿ç•™ answer_char å­—æ®µç”¨äº CAA æ³¨å…¥
        """
        def __init__(self, tokenizer):
            self.tokenizer = tokenizer
            self.collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
        
        def __call__(self, features):
            # æå– answer_charï¼ˆç”¨äº CAA æ³¨å…¥ï¼‰
            answer_chars = [f.pop("answer_char") for f in features]
            # ä½¿ç”¨æ ‡å‡† collator å¤„ç†å…¶ä»–å­—æ®µ
            batch = self.collator(features)
            # å°† answer_chars æ·»åŠ å›æ‰¹æ¬¡
            batch["answer_char"] = answer_chars
            return batch
    
    def compute_metrics(eval_pred):
        """
        è®¡ç®—éªŒè¯æŒ‡æ ‡
        
        è¿”å›éªŒè¯é›†ä¸Šçš„æŸå¤±ã€å›°æƒ‘åº¦ç­‰æŒ‡æ ‡
        """
        predictions, labels = eval_pred
        
        # predictions æ˜¯ logitsï¼Œlabels æ˜¯çœŸå®çš„ token IDs
        # è®¡ç®—å‡†ç¡®ç‡ï¼šé¢„æµ‹çš„ token ID æ˜¯å¦ä¸çœŸå®æ ‡ç­¾åŒ¹é…
        if isinstance(predictions, tuple):
            predictions = predictions[0]
        
        # ç¡®ä¿æ˜¯ numpy æ•°ç»„
        if not isinstance(predictions, np.ndarray):
            predictions = np.array(predictions)
        if not isinstance(labels, np.ndarray):
            labels = np.array(labels)
        
        # è·å–é¢„æµ‹çš„ token IDï¼ˆargmaxï¼‰
        if predictions.ndim == 3:
            # æ ‡å‡†æ ¼å¼ï¼š[batch_size, seq_len, vocab_size]
            pred_ids = np.argmax(predictions, axis=-1)
        else:
            # å¦‚æœå½¢çŠ¶ä¸å¯¹ï¼Œè¿”å› 0
            return {"eval_accuracy": 0.0}
        
        # åªè®¡ç®—éå¿½ç•¥ä½ç½®ï¼ˆlabels != -100ï¼‰çš„å‡†ç¡®ç‡
        mask = labels != -100
        if mask.sum() > 0:
            correct = (pred_ids[mask] == labels[mask]).sum()
            total = mask.sum()
            accuracy = correct / total
        else:
            accuracy = 0.0
        
        return {
            "eval_accuracy": accuracy,
        }
    
    class CustomTrainer(Trainer):
        """
        è‡ªå®šä¹‰ Trainerï¼Œæ”¯æŒ CAA æ³¨å…¥
        
        åœ¨è®¡ç®—æŸå¤±æ—¶ï¼Œå°† answer_char ä¼ é€’ç»™æ¨¡å‹ä»¥è¿›è¡Œ CAA æ³¨å…¥
        """
        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
            labels = inputs.pop("labels")
            answer_chars = inputs.pop("answer_char", None)
            
            # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆTrainer é€šå¸¸ä¼šè‡ªåŠ¨å¤„ç†ï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼‰
            input_ids = inputs["input_ids"]
            if isinstance(labels, torch.Tensor):
                # ç¡®ä¿ labels åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if labels.device != input_ids.device:
                    labels = labels.to(input_ids.device)
            
            # ä¼ é€’ answer_chars ç”¨äº CAA æ³¨å…¥
            outputs = model(
                input_ids=input_ids, 
                labels=labels, 
                target_chars=answer_chars,
                repetition_penalty=REPETITION_PENALTY
            )
            return (outputs["loss"], outputs) if return_outputs else outputs["loss"]
        
        def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
            """
            é‡å†™é¢„æµ‹æ­¥éª¤ï¼Œæ”¯æŒ CAA æ³¨å…¥å’Œ answer_char ä¼ é€’
            ğŸ”§ ä¼˜åŒ–ï¼šå‡å°‘å†…å­˜ä½¿ç”¨ï¼Œç«‹å³å°† logits ç§»åˆ° CPU
            """
            has_labels = "labels" in inputs
            inputs = self._prepare_inputs(inputs)
            
            # æå– answer_charï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            answer_chars = inputs.pop("answer_char", None)
            labels = inputs.pop("labels", None) if has_labels else None
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = model(
                    input_ids=inputs["input_ids"],
                    labels=labels,
                    target_chars=answer_chars,
                    repetition_penalty=REPETITION_PENALTY
                )
                loss = outputs["loss"] if has_labels else None
                logits = outputs["logits"]
            
            if prediction_loss_only:
                # ğŸ”§ ä¿®å¤ï¼šå¦‚æœåªéœ€è¦æŸå¤±ï¼Œç«‹å³é‡Šæ”¾ logits ä»¥èŠ‚çœå†…å­˜
                del logits
                torch.cuda.empty_cache()  # æ¸…ç† GPU ç¼“å­˜
                return (loss, None, None)
            
            # ğŸ”§ ä¿®å¤ï¼šç«‹å³å°† logits ç§»åˆ° CPU ä»¥èŠ‚çœ GPU å†…å­˜
            logits = logits.detach().cpu()
            if labels is not None:
                labels = labels.detach().cpu()
            
            return (loss, logits, labels)
        
        def _save(self, output_dir: str = None, state_dict=None):
            """é‡å†™ä¿å­˜æ–¹æ³•ï¼Œç¦ç”¨ safetensors ä»¥å¤„ç†æƒé‡å…±äº«é—®é¢˜"""
            from transformers.modeling_utils import unwrap_model
            
            output_dir = output_dir if output_dir is not None else self.args.output_dir
            os.makedirs(output_dir, exist_ok=True)
            
            # è·å–è¦ä¿å­˜çš„æ¨¡å‹
            model_to_save = unwrap_model(self.model)
            
            # ä¿å­˜æ¨¡å‹é…ç½®
            if hasattr(model_to_save, 'config'):
                model_to_save.config.save_pretrained(output_dir)
            
            # ä¿å­˜æ¨¡å‹æƒé‡ï¼Œä½¿ç”¨ pickle æ ¼å¼è€Œä¸æ˜¯ safetensors
            if state_dict is None:
                state_dict = model_to_save.state_dict()
            
            # ä½¿ç”¨ torch.save ä¿å­˜æƒé‡ï¼ˆpickle æ ¼å¼ï¼‰
            weights_file = os.path.join(output_dir, WEIGHTS_NAME)
            torch.save(state_dict, weights_file)
            
            # ä¿å­˜ tokenizerï¼ˆå¦‚æœæœ‰ï¼‰
            if self.processing_class is not None:
                self.processing_class.save_pretrained(output_dir)

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=1,  # ğŸ”§ ä¿®å¤ï¼šå‡å°è¯„ä¼°æ‰¹æ¬¡å¤§å°ä»¥é¿å…å†…å­˜æº¢å‡º
        gradient_accumulation_steps=8,
        learning_rate=2e-4,
        num_train_epochs=3,
        logging_steps=50,
        # æ›´é¢‘ç¹çš„éªŒè¯ï¼šæ¯ 200 æ­¥éªŒè¯ä¸€æ¬¡ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
        eval_strategy="steps",
        eval_steps=200,
        # ä¿å­˜ç­–ç•¥ï¼šä¿å­˜æœ€ä½³æ¨¡å‹å’Œå®šæœŸä¿å­˜
        save_strategy="steps",
        save_steps=200,
        save_total_limit=3,  # åªä¿ç•™æœ€è¿‘3ä¸ªcheckpointï¼ŒèŠ‚çœç©ºé—´
        load_best_model_at_end=True,  # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹
        metric_for_best_model="eval_loss",  # ä½¿ç”¨éªŒè¯æŸå¤±ä½œä¸ºæœ€ä½³æ¨¡å‹æŒ‡æ ‡
        greater_is_better=False,  # æŸå¤±è¶Šå°è¶Šå¥½
        fp16=True,
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        warmup_ratio=0.1,
        report_to="none",
        ddp_find_unused_parameters=False,
        remove_unused_columns=False,
        dataloader_pin_memory=False,  # ğŸ”§ ä¿®å¤ï¼šç¦ç”¨ pin_memory ä»¥èŠ‚çœå†…å­˜
    )

    # æ—©åœå›è°ƒé…ç½®
    early_stopping_callback = EarlyStoppingCallback(
        early_stopping_patience=5,  # å¦‚æœéªŒè¯æŸå¤±è¿ç»­5æ¬¡æ²¡æœ‰æ”¹å–„ï¼Œåˆ™åœæ­¢è®­ç»ƒ
        early_stopping_threshold=0.001,  # æ”¹å–„å¹…åº¦å°äº0.001è®¤ä¸ºæ²¡æœ‰æ”¹å–„
    )
    
    trainer = CustomTrainer(
        model=model_with_caa,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        processing_class=tokenizer,
        data_collator=CustomDataCollator(tokenizer),
        compute_metrics=compute_metrics,  # æ·»åŠ æŒ‡æ ‡è®¡ç®—
        callbacks=[early_stopping_callback],  # æ·»åŠ æ—©åœå›è°ƒ
    )

    # æ‰“å°è®­ç»ƒé…ç½®ä¿¡æ¯
    print("\n" + "=" * 60)
    print("è®­ç»ƒé…ç½®ä¿¡æ¯")
    print("=" * 60)
    print(f"ğŸ“Š éªŒè¯ç­–ç•¥: æ¯ {training_args.eval_steps} æ­¥éªŒè¯ä¸€æ¬¡")
    print(f"ğŸ’¾ ä¿å­˜ç­–ç•¥: æ¯ {training_args.save_steps} æ­¥ä¿å­˜ä¸€æ¬¡")
    print(f"ğŸ“ˆ æœ€ä½³æ¨¡å‹æŒ‡æ ‡: {training_args.metric_for_best_model}")
    print(f"â¹ï¸  æ—©åœæœºåˆ¶: patience={early_stopping_callback.early_stopping_patience}, threshold={early_stopping_callback.early_stopping_threshold}")
    print(f"ğŸ“¦ æœ€å¤šä¿ç•™ {training_args.save_total_limit} ä¸ª checkpoint")
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}")
    print("=" * 60)
    print("å¼€å§‹è®­ç»ƒ...")
    print("=" * 60)
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("\næ­£åœ¨ä¿å­˜æ¨¡å‹...")
    trainer.save_model(os.path.join(OUTPUT_DIR, "lora"))
    torch.save(caa.state_dict(), os.path.join(OUTPUT_DIR, "caa.bin"))
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {OUTPUT_DIR}")
    print("âœ… è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()