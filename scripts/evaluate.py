# scripts/evaluate.py
import os
import sys
import re
import torch
import json
import argparse
import random
import numpy as np
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)
from peft import LoraConfig, TaskType, get_peft_model, PeftModel
from scripts.char_adapter import CharacterAwareAdapter, load_decompose_map

# === è·¯å¾„é…ç½® ===
MODEL_PATH = "./.hf_cache_Qwen3-0.6B"
FINETUNED_MODEL_DIR = "outputs/models/qwen3-riddle-caa-lora"
TEST_DATA_PATH = "data/processed/test.json"
SRC_DATA_DIR = "data/raw/src_data"
OUTPUT_DIR = "outputs/evaluation"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === ç”Ÿæˆé…ç½® ===
REPETITION_PENALTY = 1.2

# === æ¨¡å‹åŒ…è£…å™¨ï¼ˆå‚è€ƒ train.pyï¼‰ ===
class QwenWithCAA(torch.nn.Module):
    """
    å°† Qwen æ¨¡å‹ä¸ CAA (Character-Aware Adapter) ç»“åˆçš„åŒ…è£…å™¨
    å‚è€ƒ train.py çš„å®ç°
    """
    def __init__(self, base_model, caa_adapter, tokenizer):
        super().__init__()
        self.model = base_model
        self.caa = caa_adapter
        self.tokenizer = tokenizer

    def forward(self, input_ids, labels=None, target_chars=None, repetition_penalty=1.0, **kwargs):
        """å‰å‘ä¼ æ’­ï¼Œæ”¯æŒCAAæ³¨å…¥"""
        # è·å– hidden states
        outputs = self.model.model(
            input_ids=input_ids,
            output_hidden_states=True,
            **kwargs
        )
        
        # æå–æœ€åä¸€å±‚ hidden states
        if hasattr(outputs, 'last_hidden_state'):
            hidden_states = outputs.last_hidden_state
        elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
            hidden_states = outputs.hidden_states[-1]
        else:
            hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs

        # å¦‚æœæä¾›äº† target_charsï¼Œæ³¨å…¥ CAA ä¿¡æ¯
        if target_chars is not None and labels is not None:
            answer_positions = []
            for label_row in labels:
                non_ignore = (label_row != -100).nonzero(as_tuple=True)[0]
                pos = non_ignore[-1].item() if len(non_ignore) > 0 else label_row.size(0) - 1
                answer_positions.append(pos)
            hidden_states = self.caa.inject_at_positions(hidden_states, target_chars, answer_positions)

        logits = self.model.lm_head(hidden_states)
        loss = None

        if labels is not None:
            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

        return {"loss": loss, "logits": logits} if loss is not None else {"logits": logits}

    def generate(self, input_ids=None, **kwargs):
        """å§”æ‰˜ç»™åŸºç¡€æ¨¡å‹çš„generateæ–¹æ³•"""
        if input_ids is not None:
            return self.model.generate(input_ids=input_ids, **kwargs)
        else:
            return self.model.generate(**kwargs)


def load_models():
    """åŠ è½½åŸå§‹æ¨¡å‹å’Œå¾®è°ƒåçš„æ¨¡å‹ï¼Œå‚è€ƒ train.py çš„å®ç°æ–¹å¼"""
    # æ£€æŸ¥å¿…è¦çš„è·¯å¾„
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_PATH}")
    if not os.path.exists(FINETUNED_MODEL_DIR):
        raise FileNotFoundError(f"å¾®è°ƒæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {FINETUNED_MODEL_DIR}")
    if not os.path.exists(SRC_DATA_DIR):
        raise FileNotFoundError(f"æºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {SRC_DATA_DIR}")
    
    print("æ­£åœ¨åŠ è½½tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            fix_mistral_regex=True
        )
        tokenizer.pad_token = tokenizer.eos_token
        print("âœ… Tokenizer åŠ è½½æˆåŠŸ")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½ tokenizer å¤±è´¥: {e}")

    print("æ­£åœ¨åŠ è½½åŸå§‹æ¨¡å‹...")
    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            dtype=torch.float16,
            device_map="auto",
            use_cache=True
        )
        base_model.eval()
        print("âœ… åŸå§‹æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½åŸå§‹æ¨¡å‹å¤±è´¥: {e}")

    print("æ­£åœ¨åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...")
    try:
        finetuned_base = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            dtype=torch.float16,
            device_map="auto",
            use_cache=True
        )
    except Exception as e:
        raise RuntimeError(f"åŠ è½½å¾®è°ƒæ¨¡å‹å¤±è´¥: {e}")

    # é…ç½®å¹¶åŠ è½½LoRAï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶é…ç½®ä¸€è‡´ï¼‰
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    finetuned_model = get_peft_model(finetuned_base, lora_config)
    
    # åŠ è½½LoRAæƒé‡
    lora_dir = os.path.join(FINETUNED_MODEL_DIR, "lora")
    
    if os.path.exists(lora_dir):
        try:
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨ PeftModel.from_pretrained åŠ è½½
            adapter_config_path = os.path.join(lora_dir, "adapter_config.json")
            if os.path.exists(adapter_config_path):
                print(f"ä» {lora_dir} åŠ è½½LoRAé€‚é…å™¨...")
                finetuned_model = PeftModel.from_pretrained(finetuned_base, lora_dir)
                print("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨PeftModel.from_pretrainedï¼‰")
            else:
                # æ‰‹åŠ¨åŠ è½½æƒé‡
                weights_file = os.path.join(lora_dir, "pytorch_model.bin")
                if os.path.exists(weights_file):
                    print(f"ä» {weights_file} åŠ è½½LoRAæƒé‡...")
                    state_dict = torch.load(weights_file, map_location="cpu")
                    missing_keys, unexpected_keys = finetuned_model.load_state_dict(state_dict, strict=False)
                    loaded_lora_keys = [k for k in state_dict.keys() 
                                      if any(lora_key in k for lora_key in ["lora_A", "lora_B", "lora_embedding"])]
                    if loaded_lora_keys:
                        print(f"âœ… å·²åŠ è½½ {len(loaded_lora_keys)} ä¸ªLoRAå‚æ•°")
                    else:
                        print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°LoRAæƒé‡")
                else:
                    # å°è¯•ä»æœ€æ–°çš„checkpointåŠ è½½
                    checkpoint_dirs = []
                    for item in os.listdir(FINETUNED_MODEL_DIR):
                        checkpoint_path = os.path.join(FINETUNED_MODEL_DIR, item)
                        if os.path.isdir(checkpoint_path) and item.startswith("checkpoint-"):
                            checkpoint_dirs.append((item, checkpoint_path))
                    
                    if checkpoint_dirs:
                        checkpoint_dirs.sort(key=lambda x: int(x[0].split("-")[1]), reverse=True)
                        latest_checkpoint = checkpoint_dirs[0][1]
                        checkpoint_weights = os.path.join(latest_checkpoint, "pytorch_model.bin")
                        if os.path.exists(checkpoint_weights):
                            print(f"ä» {checkpoint_weights} åŠ è½½LoRAæƒé‡...")
                            state_dict = torch.load(checkpoint_weights, map_location="cpu")
                            missing_keys, unexpected_keys = finetuned_model.load_state_dict(state_dict, strict=False)
                            loaded_lora_keys = [k for k in state_dict.keys() 
                                              if any(lora_key in k for lora_key in ["lora_A", "lora_B", "lora_embedding"])]
                            if loaded_lora_keys:
                                print(f"âœ… å·²åŠ è½½ {len(loaded_lora_keys)} ä¸ªLoRAå‚æ•°")
                            else:
                                print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°LoRAæƒé‡")
                        else:
                            print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°LoRAæƒé‡æ–‡ä»¶")
                    else:
                        print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°LoRAæƒé‡æ–‡ä»¶")
        except Exception as e:
            print(f"âŒ åŠ è½½LoRAæƒé‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            print("å°†ä½¿ç”¨æœªåŠ è½½æƒé‡çš„LoRAæ¨¡å‹è¿›è¡Œè¯„ä¼°")
    else:
        print(f"âš ï¸  è­¦å‘Š: LoRAç›®å½•ä¸å­˜åœ¨: {lora_dir}")
    
    finetuned_model.eval()

    # åŠ è½½CAAé€‚é…å™¨
    print("æ­£åœ¨åŠ è½½CAAé€‚é…å™¨...")
    caa_path = os.path.join(FINETUNED_MODEL_DIR, "caa.bin")
    decompose_map = load_decompose_map(SRC_DATA_DIR)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    caa = CharacterAwareAdapter(
        decompose_map,
        embed_dim=256,
        hidden_size=finetuned_model.config.hidden_size,
        device=device
    )
    
    if os.path.exists(caa_path):
        caa.load_state_dict(torch.load(caa_path, map_location="cpu"))
        print(f"âœ… å·²åŠ è½½CAAæƒé‡: {caa_path}")
    else:
        print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°CAAæƒé‡æ–‡ä»¶ {caa_path}")
    
    caa = caa.to(device)
    caa.eval()
    
    # åŒ…è£…æ¨¡å‹ï¼ˆå‚è€ƒ train.pyï¼‰
    model_with_caa = QwenWithCAA(finetuned_model, caa, tokenizer)
    model_with_caa.eval()

    return tokenizer, base_model, model_with_caa


def prepare_input(tokenizer, item):
    """å‡†å¤‡æ¨¡å‹è¾“å…¥ï¼Œå‚è€ƒ train.py çš„æ ¼å¼"""
    prompt = (
        "<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªå­—è°œä¸“å®¶ã€‚<|im_end|>\n"
        "<|im_start|>user\n"
        f"è°œé¢ï¼š{item['riddle']}\n"
        f"çº¿ç´¢ï¼š{item['clue']}\n"
        "è¯·ç›´æ¥å›ç­”ä¸€ä¸ªæ±‰å­—ã€‚<|im_end|>\n"
        "<|im_start|>assistant\n"
    )
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=256
    )
    
    # åˆ›å»ºå®Œæ•´åºåˆ—ï¼ˆåŒ…å«ç­”æ¡ˆï¼‰ç”¨äºè®¡ç®—loss
    full_text = prompt + item['answer'] + "<|im_end|>"
    full_inputs = tokenizer(
        full_text,
        return_tensors="pt",
        truncation=True,
        max_length=256
    )
    
    return inputs, full_inputs, item['answer']


def extract_answer(generated_text: str) -> str:
    """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–ç­”æ¡ˆæ±‰å­—ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºè°ƒè¯•
    original_text = generated_text
    
    # æ¸…ç†ç”Ÿæˆçš„æ–‡æœ¬ï¼šç§»é™¤ç‰¹æ®Šæ ‡è®°
    generated_text = generated_text.strip()
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šé¦–å…ˆç§»é™¤æ¨ç†ç›¸å…³çš„XMLæ ‡ç­¾åŠå…¶å†…å®¹ï¼ˆä½¿ç”¨DOTALLæ¨¡å¼ï¼‰
    # è¿™æ ·å¯ä»¥ç§»é™¤æ•´ä¸ªæ¨ç†å—ï¼Œé¿å…å½±å“ç­”æ¡ˆæå–
    generated_text = re.sub(r'<think>.*?</think>', '', generated_text, flags=re.DOTALL)
    generated_text = re.sub(r'<reasoning>.*?</reasoning>', '', generated_text, flags=re.DOTALL)
    
    # ç§»é™¤å…¶ä»–XMLé£æ ¼çš„æ ‡è®°ï¼ˆå•ä¸ªæ ‡ç­¾ï¼‰
    generated_text = re.sub(r'<[^>]+>', '', generated_text)
    
    # ç§»é™¤å¸¸è§çš„ç‰¹æ®Šæ ‡è®°è¯
    special_markers = ['think', 'reasoning', 'redacted', 'assistant', 'user', 'system']
    for marker in special_markers:
        generated_text = generated_text.replace(marker, '')
    
    # æ¸…ç†å¤šä½™çš„ç©ºç™½è¡Œ
    generated_text = re.sub(r'\n\s*\n+', '\n', generated_text)
    generated_text = generated_text.strip()
    
    # æå–æ‰€æœ‰ä¸­æ–‡å­—ç¬¦
    chinese_chars = [char for char in generated_text if '\u4e00' <= char <= '\u9fff']
    
    if not chinese_chars:
        return ""
    
    prediction = ""
    
    # ç­–ç•¥1: æŸ¥æ‰¾ç­”æ¡ˆæç¤ºè¯åçš„ç¬¬ä¸€ä¸ªä¸­æ–‡å­—ç¬¦ï¼ˆæ›´å…¨é¢çš„æ¨¡å¼ï¼‰
    answer_patterns = [
        r'æœ€ç»ˆç­”æ¡ˆä¸º[ï¼š:ï¼š]?\s*[ï¼š:ï¼š\n]*\s*([\u4e00-\u9fff])',  # "æœ€ç»ˆç­”æ¡ˆä¸ºï¼š"ã€"æœ€ç»ˆç­”æ¡ˆä¸ºï¼š\n\nç‰›"
        r'æœ€ç»ˆç­”æ¡ˆ[ï¼š:ï¼š]?\s*[ï¼š:ï¼š\n]*\s*([\u4e00-\u9fff])',  # "æœ€ç»ˆç­”æ¡ˆï¼š"
        r'ç­”æ¡ˆæ˜¯[ï¼š:ï¼š]?\s*([\u4e00-\u9fff])',  # "ç­”æ¡ˆæ˜¯ï¼š"ã€"ç­”æ¡ˆæ˜¯ï¼šç‰›"
        r'ç­”æ¡ˆ[ï¼š:ï¼š]?\s*[ï¼š:ï¼š\n]*\s*([\u4e00-\u9fff])',  # "ç­”æ¡ˆï¼š"ã€"ç­”æ¡ˆï¼š\nç‰›"
        r'ä¸º[ï¼š:ï¼š]?\s*([\u4e00-\u9fff])',  # "ä¸ºï¼š"ã€"ä¸º ç‰›"
        r'åº”ä¸º[ï¼š:ï¼š]?\s*([\u4e00-\u9fff])',  # "åº”ä¸ºï¼š"
        r'æ˜¯[ï¼š:ï¼š]?\s*([\u4e00-\u9fff])',  # "æ˜¯ï¼š"
        r'ï¼š\s*([\u4e00-\u9fff])',  # "ï¼šç‰›"ï¼ˆå†’å·åçš„ç¬¬ä¸€ä¸ªæ±‰å­—ï¼‰
    ]
    
    for pattern in answer_patterns:
        matches = re.finditer(pattern, generated_text)
        for match in matches:
            char = match.group(1)
            # ç¡®ä¿æå–çš„æ˜¯å•ä¸ªæ±‰å­—
            if len(char) == 1 and '\u4e00' <= char <= '\u9fff':
                prediction = char
                break
        if prediction:
            break
    
    # ç­–ç•¥2: æŸ¥æ‰¾markdownæ ¼å¼çš„ç­”æ¡ˆï¼ˆ**ç­”æ¡ˆ**ã€*ç­”æ¡ˆ*ã€ã€ç­”æ¡ˆã€‘ç­‰ï¼‰
    if not prediction:
        markdown_patterns = [
            r'\*\*([\u4e00-\u9fff])\*\*',  # **ç‰›**
            r'\*([\u4e00-\u9fff])\*',  # *ç‰›*
            r'ã€([\u4e00-\u9fff])ã€‘',  # ã€ç‰›ã€‘
            r'\[([\u4e00-\u9fff])\]',  # [ç‰›]
            r'ï¼ˆ([\u4e00-\u9fff])ï¼‰',  # ï¼ˆç‰›ï¼‰
            r'\(([\u4e00-\u9fff])\)',  # (ç‰›)
        ]
        for pattern in markdown_patterns:
            matches = re.finditer(pattern, generated_text)
            for match in matches:
                char = match.group(1)
                if len(char) == 1 and '\u4e00' <= char <= '\u9fff':
                    prediction = char
                    break
            if prediction:
                break
    
    # ç­–ç•¥3: æŸ¥æ‰¾æ–‡æœ¬æœ«å°¾çš„ç­”æ¡ˆï¼ˆé€šå¸¸ç­”æ¡ˆåœ¨æœ€åï¼‰
    if not prediction:
        # è·å–æ–‡æœ¬çš„æœ€å100ä¸ªå­—ç¬¦ï¼Œç­”æ¡ˆé€šå¸¸åœ¨è¿™é‡Œ
        text_tail = generated_text[-100:] if len(generated_text) > 100 else generated_text
        tail_chinese = [char for char in text_tail if '\u4e00' <= char <= '\u9fff']
        if tail_chinese:
            # å®šä¹‰åˆ†éš”ç¬¦é›†åˆï¼ˆæ ‡ç‚¹ã€ç©ºæ ¼ã€æ¢è¡Œç­‰ï¼‰
            separators = set([' ', '\n', '\t', 'ï¼š', ':', 'ï¼Œ', ',', 'ã€‚', '.', 'ã€', 'ï¼›', ';', 
                            'ï¼', '!', 'ï¼Ÿ', '?', 'ã€', '[', 'ã€‘', ']', 'ï¼ˆ', '(', 'ï¼‰', ')', 
                            '<', '|', '|', '>', 'ã€Š', 'ã€‹', 'ã€Œ', 'ã€'])
            # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªç‹¬ç«‹çš„æ±‰å­—ï¼ˆå‰åæœ‰åˆ†éš”ç¬¦ï¼‰
            for i in range(len(text_tail) - 1, -1, -1):
                char = text_tail[i]
                if '\u4e00' <= char <= '\u9fff':
                    # æ£€æŸ¥å‰åå­—ç¬¦ï¼Œç¡®ä¿æ˜¯ç‹¬ç«‹çš„æ±‰å­—
                    prev_char = text_tail[i-1] if i > 0 else ' '
                    next_char = text_tail[i+1] if i < len(text_tail) - 1 else ' '
                    # å¦‚æœå‰åæ˜¯åˆ†éš”ç¬¦ï¼Œå¯èƒ½æ˜¯ç­”æ¡ˆ
                    if prev_char in separators or next_char in separators:
                        prediction = char
                        break
            # å¦‚æœæ²¡æ‰¾åˆ°ç‹¬ç«‹çš„ï¼Œå°±ç”¨æœ€åä¸€ä¸ªæ±‰å­—
            if not prediction and tail_chinese:
                prediction = tail_chinese[-1]
    
    # ç­–ç•¥4: å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æ‰€æœ‰ä¸­æ–‡å­—ç¬¦ä¸­çš„æœ€åä¸€ä¸ª
    if not prediction and chinese_chars:
        prediction = chinese_chars[-1]
    
    # ç­–ç•¥5: å¦‚æœåªæœ‰ä¸€ä¸ªä¸­æ–‡å­—ç¬¦ï¼Œç›´æ¥è¿”å›å®ƒ
    if not prediction and len(chinese_chars) == 1:
        prediction = chinese_chars[0]
    
    return prediction


def calculate_perplexity(model, tokenizer, inputs, labels):
    """è®¡ç®—å›°æƒ‘åº¦"""
    with torch.no_grad():
        outputs = model(**inputs, labels=labels)
        if isinstance(outputs, dict):
            loss = outputs.get("loss")
        else:
            loss = outputs.loss
        if loss is not None:
            perplexity = torch.exp(loss).item()
        else:
            perplexity = None
    return perplexity


def generate_answer(model, tokenizer, inputs, max_new_tokens=1000, ground_truth=None):
    """ç”Ÿæˆç­”æ¡ˆï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    with torch.no_grad():
        # è·å–è®¾å¤‡
        if hasattr(model, 'model'):
            device = next(model.model.parameters()).device
        elif hasattr(model, 'parameters'):
            device = next(model.parameters()).device
        else:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        input_ids = inputs["input_ids"].to(device)
        input_length = input_ids.shape[1]
        attention_mask = inputs.get("attention_mask", None)
        if attention_mask is not None:
            attention_mask = attention_mask.to(device)
        
        # è°ƒç”¨æ¨¡å‹çš„generateæ–¹æ³•ï¼ˆä¼˜åŒ–ç”Ÿæˆå‚æ•°ï¼‰
        if hasattr(model, 'generate') and callable(getattr(model, 'generate', None)):
            generate_kwargs = {
                "input_ids": input_ids,
                "max_new_tokens": max_new_tokens,
                "do_sample": True,
                "temperature": 0.5,  # é™ä½temperatureï¼Œä½¿ç”Ÿæˆæ›´ç¨³å®š
                "top_p": 0.9,  # æ·»åŠ nucleus sampling
                "repetition_penalty": REPETITION_PENALTY,
                "pad_token_id": tokenizer.pad_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "no_repeat_ngram_size": 3,  # é¿å…é‡å¤çš„3-gram
            }
            if attention_mask is not None:
                generate_kwargs["attention_mask"] = attention_mask
            
            generated_ids = model.generate(**generate_kwargs)
        else:
            raise AttributeError(f"æ¨¡å‹ {type(model)} æ²¡æœ‰ generate æ–¹æ³•")
        
        # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        new_tokens = generated_ids[0][input_length:]
        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=False)
        
        # æå–ç­”æ¡ˆï¼ˆå…ˆæ¸…ç†å†æå–ï¼‰
        # ç§»é™¤ç‰¹æ®Štokenæ ‡è®°ï¼Œä½†ä¿ç•™å…¶ä»–å†…å®¹ç”¨äºæå–
        cleaned_text = tokenizer.decode(new_tokens, skip_special_tokens=True)
        prediction = extract_answer(cleaned_text)
        
        # å¦‚æœç¬¬ä¸€æ¬¡æå–å¤±è´¥ï¼Œå°è¯•ä»åŸå§‹æ–‡æœ¬ä¸­æå–ï¼ˆå¯èƒ½åŒ…å«ç‰¹æ®Štokenï¼‰
        if not prediction:
            prediction = extract_answer(generated_text)
        
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä»å®Œæ•´è¾“å‡ºä¸­æå–ï¼ˆåŒ…å«è¾“å…¥éƒ¨åˆ†ï¼‰
        if not prediction:
            full_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            # åªæå–assistantéƒ¨åˆ†ä¹‹åçš„å†…å®¹
            assistant_marker = "<|im_start|>assistant\n"
            if assistant_marker in full_output:
                assistant_part = full_output.split(assistant_marker)[-1]
                prediction = extract_answer(assistant_part)
        
        return prediction


def evaluate_model(model, tokenizer, test_data, model_name, use_caa=False):
    """è¯„ä¼°æ¨¡å‹"""
    print(f"\næ­£åœ¨è¯„ä¼° {model_name}...")
    
    correct = 0
    total = 0
    perplexities = []
    all_predictions = []
    all_ground_truths = []
    
    # è·å–è®¾å¤‡
    if hasattr(model, 'model'):
        device = next(model.model.parameters()).device
    elif hasattr(model, 'parameters'):
        device = next(model.parameters()).device
    else:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    for item in tqdm(test_data, desc=f"è¯„ä¼°{model_name}"):
        inputs, full_inputs, ground_truth = prepare_input(tokenizer, item)
        
        # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        inputs = {k: v.to(device) for k, v in inputs.items()}
        full_inputs = {k: v.to(device) for k, v in full_inputs.items()}
        
        # åˆ›å»ºlabelsç”¨äºè®¡ç®—å›°æƒ‘åº¦
        labels = full_inputs["input_ids"].clone()
        # åªè®¡ç®—assistantéƒ¨åˆ†çš„loss
        prompt_length = inputs["input_ids"].shape[1]
        labels[:, :prompt_length] = -100
        
        # è®¡ç®—å›°æƒ‘åº¦
        try:
            if use_caa:
                # å¯¹äºå¸¦CAAçš„æ¨¡å‹ï¼Œéœ€è¦ä¼ å…¥target_chars
                outputs = model(
                    input_ids=full_inputs["input_ids"],
                    labels=labels,
                    target_chars=[ground_truth]
                )
            else:
                outputs = model(**full_inputs, labels=labels)
            
            if isinstance(outputs, dict):
                loss = outputs.get("loss")
            else:
                loss = outputs.loss
            
            if loss is not None:
                perplexity = torch.exp(loss).item()
                # è¿‡æ»¤infå’Œnanå€¼
                if np.isfinite(perplexity) and not np.isnan(perplexity):
                    perplexities.append(perplexity)
        except Exception as e:
            print(f"è®¡ç®—å›°æƒ‘åº¦æ—¶å‡ºé”™: {e}")
            perplexity = None
        
        # ç”Ÿæˆç­”æ¡ˆ
        try:
            prediction = generate_answer(model, tokenizer, inputs, ground_truth=ground_truth)
            
            all_predictions.append(prediction)
            all_ground_truths.append(ground_truth)
            
            if prediction == ground_truth:
                correct += 1
            total += 1
        except Exception as e:
            print(f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            all_predictions.append("")
            all_ground_truths.append(ground_truth)
            total += 1
    
    accuracy = correct / total if total > 0 else 0.0
    # è¿‡æ»¤infå’Œnanå€¼åè®¡ç®—å¹³å‡å›°æƒ‘åº¦
    valid_perplexities = [p for p in perplexities if np.isfinite(p) and not np.isnan(p)]
    avg_perplexity = np.mean(valid_perplexities) if valid_perplexities else None
    
    results = {
        "model_name": model_name,
        "accuracy": accuracy,
        "correct": correct,
        "total": total,
        "avg_perplexity": avg_perplexity,
        "perplexities": perplexities,
        "predictions": all_predictions,
        "ground_truths": all_ground_truths
    }
    
    print(f"{model_name} å‡†ç¡®ç‡: {accuracy:.4f} ({correct}/{total})")
    if avg_perplexity:
        print(f"{model_name} å¹³å‡å›°æƒ‘åº¦: {avg_perplexity:.4f}")
    
    return results




def save_detailed_results(base_results, finetuned_results, save_dir):
    """ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSONæ–‡ä»¶"""
    results_summary = {
        "base_model": {
            "accuracy": base_results["accuracy"],
            "correct": base_results["correct"],
            "total": base_results["total"],
            "avg_perplexity": base_results["avg_perplexity"]
        },
        "finetuned_model": {
            "accuracy": finetuned_results["accuracy"],
            "correct": finetuned_results["correct"],
            "total": finetuned_results["total"],
            "avg_perplexity": finetuned_results["avg_perplexity"]
        },
        "improvement": {
            "accuracy_delta": finetuned_results["accuracy"] - base_results["accuracy"],
            "accuracy_relative_improvement": (finetuned_results["accuracy"] - base_results["accuracy"]) / base_results["accuracy"] * 100 if base_results["accuracy"] > 0 else 0
        }
    }
    
    # ä¿å­˜é¢„æµ‹ç»“æœï¼ˆåªä¿å­˜å‰100ä¸ªæ ·æœ¬ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
    predictions_data = []
    for i in range(min(100, len(base_results["predictions"]))):
        predictions_data.append({
            "index": i,
            "ground_truth": base_results["ground_truths"][i],
            "base_prediction": base_results["predictions"][i],
            "finetuned_prediction": finetuned_results["predictions"][i],
            "base_correct": base_results["predictions"][i] == base_results["ground_truths"][i],
            "finetuned_correct": finetuned_results["predictions"][i] == finetuned_results["ground_truths"][i]
        })
    
    results_summary["sample_predictions"] = predictions_data
    
    save_path = os.path.join(save_dir, "evaluation_results.json")
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(results_summary, f, ensure_ascii=False, indent=2)
    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {save_path}")


def main():
    # å£°æ˜å…¨å±€å˜é‡
    global MODEL_PATH, FINETUNED_MODEL_DIR, TEST_DATA_PATH, OUTPUT_DIR
    
    parser = argparse.ArgumentParser(description="è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹")
    parser.add_argument("--num_samples", type=int, default=30, help="éšæœºæŠ½å–çš„æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤30ï¼‰")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆé»˜è®¤42ï¼‰")
    parser.add_argument("--model_path", type=str, default=MODEL_PATH, help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--finetuned_dir", type=str, default=FINETUNED_MODEL_DIR, help="å¾®è°ƒæ¨¡å‹ç›®å½•")
    parser.add_argument("--test_data", type=str, default=TEST_DATA_PATH, help="æµ‹è¯•æ•°æ®è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default=OUTPUT_DIR, help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()
    
    # æ›´æ–°å…¨å±€é…ç½®
    MODEL_PATH = args.model_path
    FINETUNED_MODEL_DIR = args.finetuned_dir
    TEST_DATA_PATH = args.test_data
    OUTPUT_DIR = args.output_dir
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("=" * 60)
    print("å¼€å§‹æ¨¡å‹è¯„ä¼°")
    print("=" * 60)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print(f"\næ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {TEST_DATA_PATH}")
    with open(TEST_DATA_PATH, "r", encoding="utf-8") as f:
        test_data = json.load(f)
    print(f"æµ‹è¯•æ ·æœ¬æ€»æ•°: {len(test_data)}")
    
    # éšæœºæŠ½å–æŒ‡å®šæ•°é‡çš„æ ·æœ¬
    random.seed(args.seed)
    if len(test_data) > args.num_samples:
        sampled_data = random.sample(test_data, args.num_samples)
        print(f"éšæœºæŠ½å– {args.num_samples} ä¸ªæ ·æœ¬ï¼ˆéšæœºç§å­: {args.seed}ï¼‰")
    else:
        sampled_data = test_data
        print(f"æµ‹è¯•æ•°æ®é‡å°‘äºè¯·æ±‚æ•°é‡ï¼Œä½¿ç”¨å…¨éƒ¨ {len(test_data)} ä¸ªæ ·æœ¬")
    
    # åŠ è½½æ¨¡å‹
    tokenizer, base_model, finetuned_model = load_models()
    
    # è¯„ä¼°åŸå§‹æ¨¡å‹
    base_results = evaluate_model(
        base_model, 
        tokenizer, 
        sampled_data, 
        "åŸå§‹æ¨¡å‹",
        use_caa=False
    )
    
    # è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹
    finetuned_results = evaluate_model(
        finetuned_model,
        tokenizer,
        sampled_data,
        "å¾®è°ƒåæ¨¡å‹",
        use_caa=True
    )
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    save_detailed_results(base_results, finetuned_results, OUTPUT_DIR)
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("è¯„ä¼°æ€»ç»“")
    print("=" * 60)
    print(f"åŸå§‹æ¨¡å‹å‡†ç¡®ç‡: {base_results['accuracy']:.4f}")
    print(f"å¾®è°ƒåæ¨¡å‹å‡†ç¡®ç‡: {finetuned_results['accuracy']:.4f}")
    
    accuracy_delta = finetuned_results['accuracy'] - base_results['accuracy']
    if base_results['accuracy'] > 0:
        relative_improvement = (accuracy_delta / base_results['accuracy']) * 100
        print(f"å‡†ç¡®ç‡æå‡: {accuracy_delta:+.4f} ({relative_improvement:+.2f}%)")
    else:
        print(f"å‡†ç¡®ç‡æå‡: {accuracy_delta:+.4f} (åŸå§‹æ¨¡å‹å‡†ç¡®ç‡ä¸º0ï¼Œæ— æ³•è®¡ç®—ç›¸å¯¹æ”¹è¿›)")
    
    if base_results['avg_perplexity'] is not None and finetuned_results['avg_perplexity'] is not None:
        print(f"åŸå§‹æ¨¡å‹å¹³å‡å›°æƒ‘åº¦: {base_results['avg_perplexity']:.4f}")
        print(f"å¾®è°ƒåæ¨¡å‹å¹³å‡å›°æƒ‘åº¦: {finetuned_results['avg_perplexity']:.4f}")
        print(f"å›°æƒ‘åº¦å˜åŒ–: {finetuned_results['avg_perplexity'] - base_results['avg_perplexity']:+.4f}")
    
    print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_DIR}")
    print("=" * 60)


if __name__ == "__main__":
    main()
